name: Process Convocation PDF
on:
  repository_dispatch:
    types: [process_pdf]
concurrency:
  group: process-pdf-${{ github.event.client_payload.doc_id || github.run_id }}
  cancel-in-progress: false
jobs:
  plan:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      matrix: ${{ steps.make.outputs.matrix }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Install pdfinfo
        run: |
          sudo apt-get update
          sudo apt-get install -y poppler-utils
      - name: Download source PDF
        run: |
          curl -L "$SOURCE_URL" -o source.pdf
        env:
          SOURCE_URL: ${{ github.event.client_payload.source_url }}
      - name: Probe pages
        id: probe
        run: |
          PAGES=$(pdfinfo source.pdf | awk -F: '/Pages/ {print $2}' | xargs)
          echo "total_pages=$PAGES" >> $GITHUB_OUTPUT
      - name: Upload source artifact
        uses: actions/upload-artifact@v4
        with:
          name: source-pdf
          path: source.pdf
          overwrite: true
      - name: Make matrix
        id: make
        run: |
          python - <<'PY'
          import os, json
          total = int(os.environ['TOTAL'])
          size = int(os.environ.get('SIZE','10'))
          chunks = [{'start': i+1, 'end': min(total, i+size)} for i in range(0,total,size)]
          out = json.dumps(chunks)
          print(out)
          with open(os.environ['GITHUB_OUTPUT'],'a') as f:
            f.write('matrix='+out+'\n')
          PY
        env:
          TOTAL: ${{ steps.probe.outputs.total_pages }}
          SIZE: ${{ github.event.client_payload.chunk_size || 10 }}

  extract:
    needs: plan
    runs-on: ubuntu-latest
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install system deps
        run: |
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr poppler-utils libpoppler-cpp-dev
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Download source artifact
        uses: actions/download-artifact@v4
        with:
          name: source-pdf
          path: .
      - name: Extract chunk ${{ matrix.start }}-${{ matrix.end }} (agent)
        env:
          SOURCE_FILE: source.pdf
          ORIGINAL_FILENAME: ${{ github.event.client_payload.original_filename }}
          SESSION: ${{ github.event.client_payload.session }}
          CALLBACK_URL: ''
          CALLBACK_HMAC_SECRET: ''
          RESULT_UPLOAD_URL: ''
          RESULT_UPLOAD_TOKEN: ''
          DOC_ID: ${{ github.event.client_payload.doc_id }}
          # If user provided a specific range, honor it for a single-chunk run; otherwise use matrix chunking
          PAGE_START: ${{ github.event.client_payload.page_start || matrix.start }}
          PAGE_END: ${{ github.event.client_payload.page_end || matrix.end }}
          DPI: 300
          MIN_TEXT_WORDS: ${{ github.event.client_payload.min_text_words || 25 }}
          TESSERACT_PSM: ${{ github.event.client_payload.tesseract_psm }}
          TESSERACT_LANG: ${{ github.event.client_payload.tesseract_lang }}
        run: |
          # Run the Gemini-based agent extractor; it will create outputs/<base>.csv|xlsx
          python scripts/run_agent.py
          mkdir -p chunks
          # Move and suffix per-chunk outputs to avoid collisions
          base="$(basename "${ORIGINAL_FILENAME:-document.pdf}")"; base="${base%.*}"
          for f in outputs/${base}.*; do
            ext="${f##*.}"; mv "$f" "chunks/${base}-p${{ matrix.start }}-${{ matrix.end }}.${ext}" || true
          done
      - name: Upload chunk artifacts
        uses: actions/upload-artifact@v4
        with:
          name: chunks-${{ matrix.start }}-${{ matrix.end }}
          path: chunks/*

  aggregate:
    needs: [plan, extract]
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Download chunks
        uses: actions/download-artifact@v4
        with:
          path: chunks
          merge-multiple: true
      - name: "Preflight: list chunk CSVs"
        env:
          ORIGINAL_FILENAME: ${{ github.event.client_payload.original_filename }}
        run: |
          set -euo pipefail
          base="$(basename "${ORIGINAL_FILENAME:-document.pdf}")"
          base="${base%.*}"
          echo "[preflight] Base filename: $base"
          # Count CSVs matching the aggregator's pattern
          count=$(find chunks -type f -name "${base}-p*.csv" | wc -l | tr -d ' ')
          echo "[preflight] CSV chunk files found: $count"
          if [ "$count" -eq 0 ]; then
            echo "[preflight] No chunk CSV files found under ./chunks for base ${base}. Failing fast."
            # Show a quick tree for debugging
            find chunks -maxdepth 2 -type f | head -n 50 || true
            exit 1
          fi
          echo "[preflight] Sample of discovered CSVs:"
          find chunks -type f -name "${base}-p*.csv" | sort | head -n 20
      - name: "Preflight: connectivity check (5s timeout)"
        env:
          RESULT_UPLOAD_URL: ${{ github.event.client_payload.result_upload_url || 'https://search.riskcontrolnigeria.com/api/github/upload-results' }}
          RESULT_UPLOAD_TOKEN: ${{ secrets.RESULT_UPLOAD_TOKEN }}
          CALLBACK_URL: ${{ github.event.client_payload.callback_url || 'https://search.riskcontrolnigeria.com/api/github/callback' }}
        run: |
          set -euo pipefail
          # Helper to run a short POST and only fail on connection errors/timeouts
          check_post() {
            local url="$1"; shift
            local extra_headers=("$@")
            echo "[preflight] Checking POST connectivity: ${url}"
            # Any HTTP code is fine; only a curl transport error should fail
            code=$(curl -sS -o /dev/null -w "%{http_code}" -X POST -m 5 --connect-timeout 5 \
                   -H "Content-Type: application/json" "${extra_headers[@]}" \
                   -d '{"preflight":true,"source":"github-actions"}' "$url" || echo "curl_error")
            if [ "$code" = "curl_error" ]; then
              echo "[preflight] Connection to ${url} failed (timeout or network error within 5s). Failing fast."
              exit 1
            fi
            echo "[preflight] ${url} responded HTTP ${code} (any code OK for reachability)."
          }
          # Upload endpoint (include Authorization header if token is set)
          if [ -n "${RESULT_UPLOAD_URL:-}" ]; then
            if [ -n "${RESULT_UPLOAD_TOKEN:-}" ]; then
              check_post "${RESULT_UPLOAD_URL}" -H "Authorization: Bearer ${RESULT_UPLOAD_TOKEN}"
            else
              check_post "${RESULT_UPLOAD_URL}"
            fi
          fi
          # Callback endpoint
          if [ -n "${CALLBACK_URL:-}" ]; then
            check_post "${CALLBACK_URL}"
          fi
      - name: Aggregate and upload (merge CSVs and XLSX only)
        env:
          ORIGINAL_FILENAME: ${{ github.event.client_payload.original_filename }}
          CALLBACK_URL: ${{ github.event.client_payload.callback_url || 'https://search.riskcontrolnigeria.com/api/github/callback' }}
          CALLBACK_HMAC_SECRET: ${{ secrets.CALLBACK_HMAC_SECRET }}
          RESULT_UPLOAD_URL: ${{ github.event.client_payload.result_upload_url || 'https://search.riskcontrolnigeria.com/api/github/upload-results' }}
          RESULT_UPLOAD_TOKEN: ${{ secrets.RESULT_UPLOAD_TOKEN }}
          DOC_ID: ${{ github.event.client_payload.doc_id }}
          AGG_SKIP_DOCX: 'true'
          AGG_MAX_DOCX_ROWS: 0
        run: |
          python scripts/aggregate_results.py
      - name: Upload final artifacts
        uses: actions/upload-artifact@v4
        with:
          name: outputs
          path: outputs/*
          overwrite: true
